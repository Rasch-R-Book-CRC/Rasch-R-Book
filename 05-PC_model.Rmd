# Partial Credit Model {#PC_model}

This chapter provides a basic overview of the Partial Credit Model (PCM) [@PCM], along with guidance for analyzing data with the PCM using R [@R-base]. We use the same example data set from Chapter 4 of this book that includes participant responses to an attitude survey about science activities to illustrate the analysis using Conditional Maximum Likelihood Estimation (CMLE) via the *eRm* package [@eRm]. We also demonstrate PCM analyses using Marginal Maximum Likelihood Estimation (MMLE) and Joint Maximum Likelihood Estimation (JMLE) via the *TAM* package [@TAM]. After the analyses are complete, we present an example description of the results. The chapter concludes with a challenge exercise.

***Overview of the Partial Credit Model***

@PCM proposed the PCM for use with ordinal item responses that are scored in more than two categories (e.g., data from attitude scales or performance assessments). Similar to the Rating Scale Model [RSM; @Rating_Form; see Chapter 4), the PCM provides estimates of *person locations*, *item locations*, and *rating scale category thresholds* on a log-odds scale that represents the latent variable. However, whereas the RSM provides one set of rating scale category thresholds that are estimated using all item responses, the PCM provides separate rating scale category thresholds for each item included in the analysis. Item-specific thresholds are useful in many practical situations, including instruments that include multiple scale lengths and cases where some rating scale categories are not observed in responses to one or more items. In addition, the PCM is useful in contexts where it is important to verify comparable rating scale functioning across items, and to evaluate the measurement quality of rating scales specific to individual items.

The PCM can be stated in log-odds form as follows:

\begin{equation}\tag{5.1}ln\left[\frac{P_{n_i(xi=k)}}{P_{n_i(xi=k-1)}}\right]=\theta_{n}-\delta_{i}-\tau_{ik}\end{equation}

In the PCM, $\theta$ is the person's ability, $\delta$ is the item's difficulty, and $\tau_{ik}$ is the rating scale category threshold specific to item i. As in the RSM, the threshold is the location on the logit scale at which there is an equal probability for a rating in category $k$ and category $k - 1$. For a rating scale made up of $m$ categories, there are $m - 1$ rating scale category thresholds. Thresholds($Ï„_{k}$) are estimated separately for each element of a selected facet, such as items. They are not necessarily evenly spaced or ordered as expected.

***Rasch Model Requirements***

The PCM is based on the same requirements of unidimensionality, local independence, and invariance that we discussed in Chapter 2 for the dichotomous Rasch model. In practice, researchers should evaluate item responses for evidence that they approximate Rasch model requirements before examining model estimates in detail. Chapter 3 included details about model-data fit analysis procedures that can also be applied to the PCM. In the current chapter, we provide code for calculating some popular residual-based fit indices for items and persons based on the PCM.

## Example Data: Liking for Science

The example data set for this chapter is the same data set that we used in Chapter 4 of this book. The data include a group of 75 children's responses to the 25-item *Liking for Science* questionnaire, which was designed to measure their attitudes toward science activities. The data were originally published in @RS_analysis. Each item stem included a science activity, and three response options: 0 = *Dislike*, 1 = *Not Sure/Don't Care*, and 2 = *Like*, such that responses in higher categories indicated more-favorable attitudes toward science activities.

## PCM Analysis with CMLE in *eRm*

In the next section, we provide a step-by-step demonstration of a PCM analysis with Conditional Maximum Likelihood Estimation (CMLE) using the *eRm* package. We encourage readers to use the example data set that is provided in the online supplement to conduct the analysis along with us.

***Prepare for the Analyses***

We will use the *eRm* package [@eRm] as the first package with which we demonstrate PCM analyses. We selected *eRm* for the first illustrations in the current chapter because it includes functions for applying the PCM that are relatively straightforward to use and interpret. Please note that the *eRm* package uses the Conditional Maximum Likelihood Estimation (CMLE) method to estimate Rasch model parameters. As a result, estimates from the *eRm* package are not directly comparable to estimates obtained using other estimation methods. At the end of this chapter, we have included an illustration of RSM analyses with the *TAM* package [@TAM] with Marginal Maximum Likelihood Estimation (MMLE). We also provide an illustration with *TAM* using Joint Maximum Likelihood Estimation (JMLE), which produces comparable estimates to some popular standalone Rasch software programs, such as Winsteps [@Winsteps] and Facets [@Facets].

```{r message=FALSE,results='hide'}
#install.packages("eRm")
library("eRm")
```

Now that we have installed and loaded the package to our R session, we are ready to import the data. We will use the function `read.csv()` to import the comma-separated values (.csv) file that contains the Liking for Science survey data. We encourage readers to use their preferred method for importing data files into R or R Studio.

Please note that if you use `read.csv()` to import the data, you will first need to specify the file path to the location at which the data file is stored on your computer or set your working directory to the folder in which you have saved the data.

We will import the data using `read.csv()` and store it in an object called `science`.

```{r}
science <- read.csv("liking_for_science.csv")
```

Next, we will explore the data using descriptive statistics with the `summary()` function.

```{r}
summary(science)
```

From the summary of `science`, we can see there are no missing data. We can also get a general sense of the scales, range, and distribution of each variable in the data set. We can see that Student ID numbers range from 1 to 75, and that the maximum rating on the items was $x = 2$. Importantly, we can see that for Item 12, the minimum rating was $x = 1$, and no ratings in the first category ($x = 0$) were observed in our sample. In contrast to the RSM analysis in Chapter 4, we can include this item in our analysis with the PCM because the PCM estimates rating scale category thresholds separately for each item.

***Run the Partial Credit Model***

Next we need to isolate the item response matrix from the descriptive variables in the data (in this case, student identification numbers). To do so, we will create an object made up of only the item responses by removing the first variable (`student`) from the data.

```{r}
science.responses <- subset(science, select = -student)
```

We will use the `summary()` function to calculate descriptive statistics for the `science.responses` object to check our work and ensure that the responses are ready for analysis.

```{r}
summary(science.responses)
```

Now we are ready to run the PCM on the Liking for Science response data. We will use the `PCM()` function to run the model and store the results in an object called `PCM.science`. 

```{r}
PCM.science <- PCM(science.responses, se = TRUE)
```

When we run the PCM on the science data, we see a message indicating that the responses for item 12 have been shifted such that the lowest category is equal to zero. We need to keep this in mind when we interpret the threshold estimates for Item 12. For this item, the lowest threshold represents the threshold between $x = 1$ and $x = 2$, whereas the lowest threshold for the other items represents the threshold between $x = 0$ and $x = 1$.

***Overall Model Summary***

We will request a summary of the model results using the `summary()` function.

```{r}
summary(PCM.science)
```

The summary of the PCM output includes the Conditional Log-likelihood statistic, details about the number of iterations and model parameters, and a table with item parameter estimates, their standard errors, and confidence intervals. It is important to note that the item parameter estimatess included in this preliminary output are *item easiness* estimates--*not* item difficulty estimates We will examine item difficulty parameter estimates in detail later in our analysis.

***Wright Map***

Next, we will create a Wright Map to display our model results. This plot will provide us with an overview of the distribution of person, item, and threshold parameters. We will create the plot using the *eRm* package function `plotPImap()` on the model object (`PCM.science`). 

```{r}
plotPImap(PCM.science, main = "Liking for Science Partial Credit Model Wright Map")
```

In this *Wright Map* display, the results from the PCM analysis of the Liking for Science data are summarized graphically. The figure is organized as follows:

Starting at the bottom of the figure, the horizontal axis (labeled *Latent Dimension*) is the logit scale that represents the latent variable. In the application of the Liking for Science data, lower numbers indicate less-favorable attitudes toward science activities, and higher numbers indicate more-favorable attitudes toward science activities. 

The central panel of the figure shows item difficulty locations on the logit scale for the 25 Liking for Science items; the y-axis for this panel shows the item labels. By default in *eRm*, the items are ordered according to their original order in the response matrix. The items can be ordered by difficulty by adding `sorted = TRUE` as an argument in the `plotPImap()` call. For each item, a solid circle plotting symbol shows the overall location estimate. This solid circle symbol is connected to two open-circle symbols that show the locations of the rating scale category thresholds. Each threshold is labeled with either a $1$ to indicate the threshold between rating scale category $x = 0$ and $x = 1$, or a $2$ to indicate the threshold between rating scale category $x = 1$ and $x = 2$. Because the PCM estimates thresholds separately for each item, the distance between the thresholds varies across items. In addition, an asterisk symbol (*) is shown to the right of the central panel of the Wright map that corresponds to item 13. For this item, the thresholds were disordered.

The upper panel of the figure shows a histogram of person (in this case, children) location estimates on the logit scale. Small vertical lines on the x-axis of this histogram show the points on the logit scale at which information (variance) is maximized for the sample of persons and items in the analysis. These lines can be omitted by adding `irug = FALSE` as an argument in the `plotPImap()` function.

***Item Parameters***

Next, we will examine the item parameter estimates. In the PCM, item difficulty and threshold locations are combined. The *eRm* package reports item-specific rating scale category threshold parameters for each item as part of the `PCM()` function that we used to estimate the model. We extract these parameters, print them to the console, and calculate summary statistics for them with the following code.

```{r}
item.locations <- PCM.science$etapar
item.locations
```

```{r}
summary(item.locations)
```

Because of the nature of the estimation process used in *eRm*, the item.locations object that we just created does not include the location estimate for the first threshold for the first item. One can calculate the location for item 1, threshold 1 by subtracting the sum of the item parameter estimates from zero. In the following code, we find the location for item 1 threshold 1, and then create a new object with all of the item + threshold locations.

```{r}
i1 <- 0 - sum(item.locations[(1:length(item.locations)) - 1])
item.locations.all <- c(i1, item.locations[(1:length(item.locations)) - 1])
item.locations.all[1]
```

Alternatively, one can apply the `thresholds()` function to the model object to find the item locations from the PCM. This procedure provides item location estimates ($\delta$) as well as estimates of the item location combined with rating scale category thresholds ($\delta$ + $\tau$). However, the values produced with this function are not centered at zero logits, so a little manipulation is required to obtain the centered values. In the following code, we apply the `thresholds()` function to obtain the uncentered item location estimates and then calculate centered item locations.

```{r}
# Apply thresholds() function to the model object in order to obtain item locations (not centered at zero logits):
items.and.taus <- thresholds(PCM.science)
items.and.taus.table <- as.data.frame(items.and.taus$threshtable)
uncentered.item.locations <- items.and.taus.table$X1.Location

# Set the mean of the item locations to zero logits:
centered.item.locations <- scale(uncentered.item.locations, scale = FALSE)

# Summarize the results:
summary(centered.item.locations)
```

It is important to note that the eta parameter estimates from the PCM object include *cumulative* rating scale category thresholds. Cumulative thresholds are not used in all Rasch model applications. Instead, many researchers use Rasch-Andrich (i.e., adjacent categories) thresholds [@Prob_PM; @Conceptual_notes]. We will calculate the values of the adjacent-categories rating scale category thresholds using the results from the `thresholds()` function.

In our example, we have a rating scale with three categories, so we have two rating scale category thresholds for all of the items in which all categories were observed. In the following code chunk, we create an empty object in which to store the threshold estimates (`tau.matrix`), and then we use two for-loops to calculate the estimates and store them in our object. 

```{r}
# Specify the number of items that were included in the analysis:
n.items <- ncol(science.responses)

# Specify the number of thresholds as the maximum observed score in the response matrix (be sure the responses begin at category 0):
n.thresholds <- max(science.responses)

# Create a matrix in which to store the adjacent-category threshold values for each item:
tau.matrix <- matrix(data = NA, ncol = n.thresholds, nrow = n.items)

# Calculate adjacent-category threshold values:
for(item.number in 1:n.items){
  for(tau in 1:n.thresholds){
    tau.matrix[item.number, tau] <- (items.and.taus.table[item.number, (1+tau)] -    
                                       items.and.taus.table[item.number,1])[1]
  }
}
```

We can examine the threshold results for evidence that the rating scale categories are ordered as expected according to the ordinal rating scale. When scale categories are ordered as expected, the first threshold has a lower location on the latent variable compared to the second threshold, and so on. In our example with the Liking for Science data, we have two thresholds, so we need to evaluate the expression $\tau_1$ <= $\tau_2$. We check this property using the following code, which compares the first threshold estimate (saved in the first column of `tau.matrix`) to the second threshold estimate (saved in the second column of `tau.matrix`). Analysts whose scales include more rating scale categories should evaluate this property for all pairs of adjacent thresholds.

```{r}
tau.matrix[,1] <= tau.matrix[,2]
```

For Item 12, the expression returns a value of "NA" because only one threshold was estimated for this item. The results indicate that $\tau_1$ <= $\tau_2$ is true for all items except Item 13. The disordered thresholds for Item 13 indicate that lower locations on the construct were required to provide a rating in category 1 compared to category 2, when the opposite order would have been expected given the rating scale category labels. This result suggests that additional research may be required to understand students' responses to Item 13. 

Next, we will calculate standard errors for each item + threshold location and store them in an object called `delta.tau.se`. We will use `summary()` to examine descriptive statistics for each item + threshold standard error.

```{r}
delta.tau.se <- items.and.taus$se.thresh
summary(delta.tau.se)
```

***Item Response Functions***

We will examine graphical displays of item difficulty using item response functions. With the PCM, the *eRm* package creates plots of the probability for a rating in each rating scale category, conditional on person locations on the latent variable. In the following code, we use `plotICC()` from *eRm* to create rating scale category probability plots for the first 5 items in our analysis. We included `ask = FALSE` in our function call to generate all of the plots at once. Readers who need to specify plots for different items can update the `items=` specification to produce the desired plots.

```{r}
plotICC(PCM.science, ask = FALSE, item.subset = c(1:5))
```

In each item-specific plot, the x-axis is the logit scale that represents the latent variable; in our example, this scale represents favorability toward science activities. The y-axis is the probability for a rating in each category, conditional on person locations on the latent variable. Separate lines in different colors show the conditional probability for each category of responses observed for the item of interest. Because we used the PCM, the overall shape of the curves and the relative distance between the curves is unique for each items--reflecting the individual set of rating scale category thresholds for each item. In addition, the location of the curves on the x-axis shifts to reflect each item's overall difficulty level.

To supplement the numeric evaluation of rating scale category ordering, analysts can examine plots of rating scale category probabilities for evidence that the categories are ordered as expected. Specifically, the probabilities associated with each item should increase and decrease in the expected order as locations on the latent variable progress from low to high. In addition, analysts can examine these plots for evidence that each category describes a unique range of locations on the latent variable. In other words, each category should be the most-probable response for some range of locations on the latent variable. Distinct category probability curves provide evidence that the rating scale categories can distinguish among examinees with different locations on the latent variable. In cases where such distinctions are not observed, researchers may consider revising the scale to include fewer categories prior to further analysis, as well as making revisions to the scale length or category labels prior to future administrations. For details about these procedures and guidance in decision-making related to rating scale revisions, we recommend that readers consult @Apply_RM ,chapter 11, as well as @Optimizing_RS.

**_Note about item fit_**

In the *eRm* package, it is necessary to calculate person parameters *before* item fit statistics can be calculated. Accordingly, we will proceed with a brief examination of person parameters before we conduct item fit analyses. In practice, we recommend examining item fit before examining and interpreting location estimates in detail.

**Person Parameters***

The next step in our analysis is to calculate and examine person location parameters (i.e., person achievement or ability estimates). With the CMLE method that is used in eRm, person parameters are calculated *after* the item locations are estimated.

In the following code, we calculate person locations that correspond to our model using the `person.parameter()` function with the PCM model object (`PCM.science`). This function also produces standard errors for the person locations. We store the person location estimates and their standard errors in a new data frame called `person.locations`, and then request a summary of the estimation results using the `summary()` function.

```{r}
# Calculate person parameters:
person.locations.estimate <- person.parameter(PCM.science)

# Store person parameters and their standard errors in a dataframe object:
person.locations <- cbind.data.frame(person.locations.estimate$thetapar,
                                     person.locations.estimate$se.theta)
names(person.locations) <- c("theta", "SE")

# View summary statistics for person parameters:
summary(person.locations)
```

The estimation procedure in *eRm* does not directly produce parameter estimates for persons with extreme scores. In our example, extreme scores would result from a child giving a response of $x = 0$ to all items or a child giving a response of $x = 2$ to all items. For these children, a standard error is not calculated. In our example, Child 2 had an extreme score because they gave a rating of $2$ to all items.

***Item Fit***

Next, we will conduct a brief exploration of item fit statistics for the Liking for Science items. We considered item fit in detail in Chapter 3; readers can use the procedures in that chapter to examine item fit in detail for the PCM.

To calculate numeric item fit statistics, we will use the function `itemfit()` from *eRm* on the person parameter object (`person.locations.estimate`). This function produces several item fit statistics, including infit mean square error ($MSE$), outfit $MSE$, and standardized infit and outfit statistics. We will store the item fit results in a new object called `item.fit`, and then format this object as a data.frame for easy manipulation and exporting.

```{r}
item.fit.results <- itemfit(person.locations.estimate)

item.fit <- cbind.data.frame(item.fit.results$i.infitMSQ,
                             item.fit.results$i.outfitMSQ,
                             item.fit.results$i.infitZ,
                             item.fit.results$i.outfitZ)

names(item.fit) <- c("infit_MSE", "outfit_MSE", "std_infit", "std_outfit")
```

Next, we will request a summary of the numeric item fit statistics using the *summary()* function.

```{r}
summary(item.fit)
```

The `item.fit` object includes mean square error ($MSE$) and standardized ($Z$) versions of the infit and outfit statistics for each item included in the analysis. These statistics are summaries of the residuals associated with each item. When data fit Rasch model expectations, the $MSE$ versions of infit and outfit are expected to be close to 1.00 and the standardized versions of infit and outfit are expected to be around 0.00. Please refer to Chapter 3 for a detailed discussion of item fit.

***Person Fit***

Next, we will conduct a brief exploration of person fit statistics. To calculate numeric person fit statistics, we will apply the function `personfit()` from *eRm* to the person parameter object (`person.locations.estimate`). This function produces several person fit statistics, including infit $MSE$, outfit $MSE$, and standardized infit and outfit $MSE$ statistics. We will store the person fit results in a new object called `person.fit`, and then format this object as a data.frame for easy manipulation and exporting.

```{r}
person.fit.results <- personfit(person.locations.estimate)

person.fit <- cbind.data.frame(person.fit.results$p.infitMSQ,
                             person.fit.results$p.outfitMSQ,
                             person.fit.results$p.infitZ,
                             person.fit.results$p.outfitZ)

names(person.fit) <- c("infit_MSE", "outfit_MSE", "std_infit", "std_outfit")
```

Next, we will request a summary of the numeric person fit statistics using the `summary()` function.

```{r}
summary(person.fit)
```

The `person.fit` object includes $MSE$ and standardized ($Z$) versions of the infit and outfit statistics for each person. 

Next, we calculate the reliability of separation statistics for persons and items.

```{r}
## Person separation reliability
person.separation.reliability <- SepRel(person.locations.estimate)
person.separation.reliability
```

```{r}
## Item separation reliability:

# Get Item scores
ItemScores <- colSums(science.responses)

# Get Item SD
ItemSD <- apply(science.responses,2,sd)

# Calculate the SE of the Item
ItemSE <- ItemSD/sqrt(length(ItemSD))

# Compute the Observed Variance (also known as Total Person Variability or Squared Standard Deviation)
SSD.ItemScores <- var(ItemScores)

# Compute the Mean Square Measurement error (also known as Model Error variance)
Item.MSE <- sum((ItemSE)^2) / length(ItemSE)

# Compute the Item Separation Reliability
item.separation.reliability <- (SSD.ItemScores-Item.MSE) / SSD.ItemScores

item.separation.reliability
```

## Summarize the Results in Tables

As a final step, we will create tables that summarize the calibrations of the persons, items, and rating scale category thresholds.

Table 1 provides an overview of the logit scale locations, standard errors, fit statistics, and reliability statistics for items and persons. This type of table is useful for reporting the results from Rasch model analyses because it provides a quick overview of the location estimates and numeric model-data fit statistics for the items and persons in the analysis.

```{r}
PCM_summary.table.statistics <- c("Logit Scale Location Mean",
                              "Logit Scale Location SD",
                              "Standard Error Mean",
                              "Standard Error SD",
                              "Outfit MSE Mean",
                              "Outfit MSE SD",
                              "Infit MSE Mean",
                              "Infit MSE SD",
                              "Std. Outfit Mean",
                              "Std. Outfit SD",
                              "Std. Infit Mean",
                              "Std. Infit SD",
                              "Separation.reliability")
                              
PCM_item.summary.results <- rbind(mean(centered.item.locations),
                              sd(centered.item.locations),
                              mean(delta.tau.se),
                              sd(delta.tau.se),
                              mean(item.fit.results$i.outfitMSQ),
                              sd(item.fit.results$i.outfitMSQ),
                              mean(item.fit.results$i.infitMSQ),
                              sd(item.fit.results$i.infitMSQ),
                              mean(item.fit.results$i.outfitZ),
                              sd(item.fit.results$i.outfitZ),
                              mean(item.fit.results$i.infitZ),
                              sd(item.fit.results$i.infitZ),
                              item.separation.reliability)

PCM_person.summary.results <- rbind(mean(person.locations$theta),
                              sd(person.locations$theta),
                              mean(person.locations$SE),
                              sd(person.locations$SE),
                              mean(person.fit$outfit_MSE),
                              sd(person.fit$outfit_MSE),
                              mean(person.fit$infit_MSE),
                              sd(person.fit$infit_MSE),
                              mean(person.fit$std_outfit),
                              sd(person.fit$std_outfit),
                              mean(person.fit$std_infit),
                              sd(person.fit$std_infit),
                              as.numeric(person.separation.reliability))

# Round the values for presentation in a table:
PCM_item.summary.results_rounded <- round(PCM_item.summary.results, digits = 2)

PCM_person.summary.results_rounded <- round(PCM_person.summary.results, digits = 2)

PCM_Table1 <- cbind.data.frame(PCM_summary.table.statistics,
                           PCM_item.summary.results_rounded, 
                           PCM_person.summary.results_rounded)

# Add descriptive column labels:
names(PCM_Table1) <- c("Statistic", "Items", "Persons")  
```

Table 2 summarizes the overall calibrations of individual items. For data sets with manageable sample sizes such as the Liking for Science data example in this chapter, we recommend reporting details about each item in a table similar to this one.

```{r}
# Calculate the average rating for each item:
Avg_Rating <- apply(science.responses, 2, mean)

# Combine item calibration results in a table:

PCM_Table2 <- cbind.data.frame(c(1:ncol(science.responses)), 
                           Avg_Rating,
                           centered.item.locations,
                           tau.matrix,
                           item.fit$outfit_MSE,
                           item.fit$std_outfit,
                           item.fit$infit_MSE,
                           item.fit$std_infit)

# Add descriptive column labels:
names(PCM_Table2) <- c("Task ID", "Average Rating", "Item Location","Threshold 1", "Threshold 2", "Outfit MSE","Std. Outfit", "Infit MSE","Std. Infit")                           

# Sort Table 2 by Item difficulty:
PCM_Table2 <- PCM_Table2[order(-PCM_Table2$`Item Location`),]

# Round the numeric values (all columns except the first one) to 2 digits:
PCM_Table2[, -1] <- round(PCM_Table2[,-1], digits = 2)
```

Finally, Table 3 summarizes person calibrations. When there is a relatively large person sample size, it may be more useful to present the results as they relate to individual persons or subsets of the person sample as they are relevant to the purpose of the analysis.

In our person calibration table, we have included the results for all of the children with non-extreme scores. This includes all of the children in our sample except Child #2.

```{r}
# Calculate average rating for persons who did not have extreme scores
Person_Avg_Rating <- apply(person.locations.estimate$X.ex,1, mean)

# Combine person calibration results in a table:
PCM_Table3 <- cbind.data.frame(rownames(person.locations),
                          Person_Avg_Rating,
                          person.locations$theta,
                          person.locations$SE,
                          person.fit$outfit_MSE,
                          person.fit$std_outfit,
                          person.fit$infit_MSE,
                          person.fit$std_infit)
                          
# Add descriptive column labels:
names(PCM_Table3) <- c("Child ID", "Average Rating", "Person Location","Person SE","Outfit MSE","Std. Outfit", "Infit MSE","Std. Infit")

# Round the numeric values (all columns except the first one) to 2 digits:
PCM_Table3[, -1] <- round(PCM_Table3[,-1], digits = 2)
```

## PCM Application with MMLE in *TAM*

The next section includes an illustration of PCM analyses with the *Test Analysis Modules* or *TAM* package [@TAM] with Marginal Maximum Likelihood Estimation (MMLE). After this illustration, we also demonstrate the use of *TAM* to estimate the PCM with Joint Maximum Likelihood Estimation (JMLE). These illustrations use the Liking for Science data set that we described earlier.

Except where there are significant differences between the *eRm* and *TAM* procedures, we provide fewer details about the analysis procedures and interpretations in this section compared to the first illustration.

***Prepare for the Analyses***

To get started with the *TAM* package, install and load it into your R environment using the following code.

```{r message=FALSE,results='hide'}
#install.packages("TAM")
library("TAM")
```

To facilitate the example analysis, we will also use the *WrightMap* package [@WrightMap]:

```{r message=FALSE,results='hide'}
# install.packages("WrightMap")
library("WrightMap")
```

If you have not already imported the Liking for Science data and isolated the response matrix, please do so before continuing with the *TAM* analyses.  

***Run the Partial Credit Model***

To obtain Rasch-Andrich thresholds (i.e., adjacent-categories thresholds) from our analysis, we need to generate a design matrix for the model that includes specifications for those parameters. We will do this using the `designMatrices()` function from *TAM* and save the result in a new object called `design.matrix`.

```{r}
design.matrix <- designMatrices(resp=science.responses, modeltype="PCM", constraint = "items")$A
```

Now we can run the PCM with our design matrix using the `tam.mml()` function with several specifications.

```{r results="hide"}
PCM.science_MMLE <- tam.mml(science.responses, irtmodel="PCM", A = design.matrix, constraint = "items", verbose = FALSE) 
```

Next, we will request a summary of the model results using the `summary()` function.

```{r}
summary(PCM.science_MMLE)
```

The summary of the PCM output includes details about the number of iterations, global model fit statistics, a summary of the model parameters, and several other statistics.

***Item Parameters***

Next, we will examine the item difficulty location and rating scale category threshold estimates. As of this writing, the *TAM* package does not provide centered item estimates (mean set to zero logits) with the design matrix that we specified. As a result, we need to manually center the item locations at zero logits for ease in interpretation. We will use the same procedure that we used earlier to do the centering.

First, we need to extract the item location estimates from the model object (`PCM.science_MMLE`). The item locations are stored in the `item_irt` component of the model object, so we will extract them using the `$` operator. The `item_irt` table includes other item parameters as well, including threshold estimates. We will examine those later. For now, we will save the overall item locations in an object called `items_MMLE`.

```{r}
items_MMLE <- PCM.science_MMLE$item_irt$beta
```

Next, we will center the item parameter estimates that are stored in `items_MMLE` at zero logits, and request summary statistics for the estimates to check our work.

```{r}
uncentered.item.locations_MMLE <- items_MMLE

centered.item.locations_MMLE <- scale(uncentered.item.locations_MMLE, scale = FALSE)

summary(centered.item.locations_MMLE)
```

We need to find the rating scale category threshold parameter estimates for our rating scale. In our example with the Liking for Science data, the rating scale has three categories, so there are two threshold parameters for each item in which all three categories are observed. The following code extracts the adjacent-categories threshold parameters from the `item_ parameter table_irt` component of the `PCM.science_MMLE` object, and then stores them in an object called `tau.estimates_MMLE`.

```{r}
# View the item parameter table (note that the overall item location shown here is not centered):
PCM.science_MMLE$item_irt
```

```{r}
# Save only the threshold estimates, which begin in column 4:
tau.estimates_MMLE <- PCM.science_MMLE$item_irt[, c(4 : (3+n.thresholds))]

# View the threshold estimates:
tau.estimates_MMLE
```

Note that the threshold values shown in the `tau.estimates_MMLE` must be added to the overall item location to obtain the location on the logit scale at which there is an equal probability for a rating in the corresponding adjacent categories. These values correspond to the graphical displays of rating scale category probabilities for the PCM.

As we discussed in the *eRm* illustration earlier in this chapter, it is important to examine threshold parameters for each item to ensure that the rating scale categories are ordered as expected, and that there is sufficient distance between the thresholds to describe a distinct range of locations on the latent variable.

***Item Response Functions***

Next, we will examine rating scale category probability plots for the items in our analysis. To save space, we have only printed the plots for the first five items here. Analysts can request plots for different items using the `items=` argument in the `plot()` function.

```{r}
plot(PCM.science_MMLE, type="items", items = c(1:5))
```

This code generates plots of rating scale category probabilities for each item. These plots have the same interpretation as the rating scale category probability plots that we generated using *eRm*, where the x-axis is the logit scale that represents the latent variable, the y-axis is the probability for a rating in each category, and individual lines show the conditional probability for a rating in each category. As we discussed earlier, analysts can examine these plots for evidence of category ordering and distinctiveness.

***Item Fit***

Next, we will examine numeric and graphical item fit indices using the `itemfit()` function from *TAM*. We will save the results in an object called `item.fit_MMLE`.

```{r results="hide"}
MMLE_fit <- msq.itemfit(PCM.science_MMLE)
item.fit_MMLE <- MMLE_fit$itemfit
```

Next, we will view summary statistics for the fit statistics.
```{r}
summary(item.fit_MMLE)
```

The `tam.fit()` function provides mean square error ($MSE$) and standardized ($t$) versions of the infit and outfit statistics for Rasch models. The `Infit` and `Outfit` statistics are the $MSE$ versions and the `Infit_t` and `Outfit_t` statistics are the standardized versions of the statistics. *TAM* also reports a $p$ value for the standardized fit statistics (`Infit_p` and `Outfit_p`), along with adjusted significance values (`Infit_pholm` and `Outfit_pholm`). Please see Chapter 3 for a consideration of procedures for evaluating item fit in detail, including the use of graphical tools to evaluate item fit.

***Person Parameters***

Now we will examine person parameter estimates. With the MMLE procedure in *TAM*, person parameters are calculated after the item estimates using the `tam.wle()` function. The following code calculates person parameter estimates and saves them in an object called `person.locations_MMLE`.

```{r}
# Use the tam.wle function to calculate person location parameters:
person.locations.estimate_MMLE <- tam.wle(PCM.science_MMLE)

# Store person parameters and their standard errors in a dataframe object:
person.locations_MMLE <- cbind.data.frame(person.locations.estimate_MMLE$theta,
                                     person.locations.estimate_MMLE$error)
# Add descriptive column labels:
names(person.locations_MMLE) <- c("theta", "SE")

# View summary statistics for person parameters:
summary(person.locations_MMLE)
```

Because we centered the item location estimates at zero logits earlier, we need to adjust the person location estimates so that they can be compared to the centered item locations. We will do this by subtracting the original (uncentered) item mean from the person locations.

```{r}
# Subtract the original (uncentered) item mean location from the person locations:
person.locations_MMLE$theta_adjusted <- person.locations_MMLE$theta - mean(uncentered.item.locations_MMLE)

# Summary of person location estimates:
summary(person.locations_MMLE)
```

If analysts do not prefer to use the zero-centered item location estimates, they should also use the original person parameters without subtracting the mean item location.

***Person Fit***

We can evaluate person fit using the `tam.personfit()` function from *TAM*. This function uses the model object as an argument and it produces outfit and infit statistics, as well as standardized versions of these statistics, for each person in the response matrix.

```{r}
person.fit.results_MMLE <- tam.personfit(PCM.science_MMLE)

summary(person.fit.results_MMLE)
```

***Wright Map***

Next, we will create a Wright Map from our model results. This display will provide us with an overview of the distribution of person, item, and threshold parameters. We will create the plot by applying the *WrightMap* package function `IRT.WrightMap()` to the model object (`PCM.science_MMLE`). 

For ease in interpretation, we will use the centered item and person locations that we calculated in this analysis. We need to specify these modified parameter estimates in the `WrightMap()` function. The following code prepares the parameter estimates and uses them to plot the Wright Map. 

```{r}
# Combine centered item estimates with thresholds:
n.items <- ncol(science.responses)

thresholds_MMLE <- matrix(data = NA, nrow = n.items, ncol = n.thresholds)

for(item.number in 1:n.items){
  for(tau in 1:n.thresholds){
  thresholds_MMLE[item.number, tau] <- centered.item.locations_MMLE[item.number] +
      tau.estimates_MMLE[item.number, tau]
  } 
}

thetas_MMLE <- person.locations_MMLE$theta_adjusted

# Plot the Wright Map
wrightMap(thetas = thetas_MMLE,
        thresholds = thresholds_MMLE,
         main.title = "Liking for Science Partial Credit Model Wright Map (MMLE)",
        show.thr.lab	= TRUE, dim.names = "",
        label.items.rows= 2)
```

In this *Wright Map* display, the results from the PCM analysis of the Liking for Science data are summarized graphically. The figure is organized as follows:

The left panel of the plot shows a histogram of respondent (children) locations on the logit scale that represents the latent variable. Units on the logit scale are shown on the far-right axis of the plot (labeled *Logits*). 

The large central panel of the plot shows the rating scale category threshold estimates specific to each item on the logit scale that represents the latent variable. Light gray diamond shapes show the logit scale location of the threshold estimates for each item, as labeled on the x-axis. Thresholds are labeled using tau symbols followed by an integer that shows the threshold number. In our example, $\tau_1$ is the threshold between rating scale categories $x = 0$ and $x = 1$, and $\tau_2$ is the threshold between rating scale categories $x = 1$ and $x = 2$.

Even though it is not appropriate to fully interpret item and person locations on the logit scale until there is evidence of acceptable model-data fit, we recommend examining the Wright Map during the preliminary stages of an item analysis to get a general sense of the model results and to identify any potential scoring or data entry errors.

A quick glance at the Wright Map suggests that, on average, the persons are located higher on the logit scale compared to the average item threshold locations. In addition, there appears to be a relatively wide spread of person and item locations on the logit scale, such that the Liking for Science questionnaire appears to be a useful tool for identifying differences in children's attitudes toward science activities as well as the difficulty to find each of the activities as favorable. Finally, the distance between rating scale category thresholds displays notable variability across the items in the survey. This finding supports the use of the PCM to analyze the Liking for Science data.

## PCM Application with JMLE in *TAM*

In the following section, we provide an illustration of PCM analyses with the *Test Analysis Modules* or *TAM* package [@TAM] with Joint Maximum Likelihood Estimation (JMLE). Except where there are significant differences between the MMLE and JMLE procedures, we provide fewer details about the analysis procedures and interpretations compared to the *eRm* and *TAM* MMLE illustrations.

***Prepare for the Analyses***

Please make sure that you have installed the *TAM* and *WrightMap* packages and loaded them into your working environment. You will also need to have imported the Liking for Science data and isolated the response matrix as described earlier in the chapter before you run the R code in the following sections.

***Run the Partial Credit Model***

We will use the `tam.jml()` function to run the PCM with JMLE and store the results in an object called `PCM.science_JMLE`. As with the MMLE procedure, in order to obtain Rasch-Andrich thresholds (i.e., adjacent-categories thresholds), we need to generate a design matrix for the model that includes specifications for those parameters. We will do this using the `designMatrices()` function from *TAM* and save the result in a new object called `design.matrix`.

```{r}
design.matrix <- designMatrices(resp=science.responses, modeltype="PCM", constraint = "items")$A
```

Now we can run the PCM with our design matrix. After we run the model, we will request a summary of the model results using the `summary()` function.

```{r results="hide"}
PCM.science_JMLE <- tam.jml(science.responses, A = design.matrix, constraint = "items", control=list(maxiter=500), version=2 , verbose = FALSE)
```

```{r}
summary(PCM.science_JMLE)
```

***Item Parameters***

Next, we will examine the item difficulty location and rating scale category threshold estimates. As described earlier, we need to manually center the item locations at zero logits for ease in interpretation. 

With the JMLE procedure, the overall item locations are stored in the `xsi.item` column of the the `item` table within the model object, which we will extract using the `$` operator.

```{r}
items_JMLE <- PCM.science_JMLE$item$xsi.item
```

Next, we will center the item parameter estimates that are stored in `items_JMLE` at zero logits, and request summary statistics for the estimates to check our work.

```{r}
uncentered.item.locations_JMLE <- items_JMLE

centered.item.locations_JMLE <- scale(uncentered.item.locations_JMLE, scale = FALSE)

summary(centered.item.locations_JMLE)
```

We need to find the rating scale category threshold parameter estimates. The following code extracts the adjacent-categories threshold parameters from the item parameter table (`item`) that is stored in the `PCM.science_JMLE` object, and then stores them in an object called `tau.estimates_JMLE`.

```{r}
# Specify the number of thresholds as the maximum observed score in the response matrix (be sure the responses begin at category 0):
n.thresholds <- max(science.responses)

# Save the threshold estimates, which begin in column 5 of the item table:
tau.estimates_JMLE <- PCM.science_JMLE$item[, c(5 : (4+n.thresholds))]

# Specify the number of items that were included in the analysis:
n.items <- ncol(science.responses)

# Create a matrix in which to store the adjacent-category threshold values for each item:
tau.matrix_JMLE <- matrix(data = NA, ncol = n.thresholds, nrow = n.items)

# Calculate adjacent-category threshold values:

for(item.number in 1:n.items){
  for(tau in 1:n.thresholds){
    tau.matrix_JMLE[item.number, tau] <- ifelse(tau == 1,
                                    (tau.estimates_JMLE[item.number, tau] -  
                                       uncentered.item.locations_JMLE[item.number]),
                                                
                                        (tau.estimates_JMLE[item.number, tau] -
                                          sum(tau.estimates_JMLE[item.number, c((tau-1))]) - 
                                          uncentered.item.locations_JMLE[item.number]))
  }
}

# View the threshold estimates:
tau.matrix_JMLE
```

***Item Response Functions***

Next, we will examine rating scale category probability plots. These plots have the same format and interpretation as in the previous examples in this chapter. To save space, we only printed the plots for the first five items here. Analysts can request plots for different items using the `items = ` argument in the `plot()` function.

```{r}
plot(PCM.science_JMLE, type="items", items = c(1:5))
```

***Item Fit***

We will examine numeric item fit indices using the `itemfit()` function from *TAM*. We will save the results in an object called `item.fit_MMLE` and then view summary statistics for the fit statistics.

```{r}
JMLE_fit <- tam.fit(PCM.science_JMLE)

item.fit_JMLE <- JMLE_fit$fit.item
summary(item.fit_JMLE)
```
The `tam.fit()` function provides mean square error ($MSE$) and standardized ($t$) versions of the outfit and infit statistics for Rasch models for each item estimate.

***Person Parameters***

Now we will examine person parameter estimates. With the JMLE procedure in *TAM*, person parameters are calculated at the same time as the item parameters. This means that we can extract the person parameter estimates from the model object without any additional iterations. The following code calculates person parameter estimates and saves them in an object called `person.locations_JMLE`.

```{r}
# Store person parameters and their standard errors in a data.frame object:
person.locations_JMLE <- cbind.data.frame(PCM.science_JMLE$theta,
                                     PCM.science_JMLE$errorWLE)

names(person.locations_JMLE) <- c("theta", "SE")

# View summary statistics for person parameters:
summary(person.locations_JMLE)
```

Because we centered the item location estimates at zero logits earlier, we need to adjust the person location estimates so that they can be compared to the centered item locations. We will do this by subtracting the original (uncentered) item mean from the person locations.

```{r}
# Subtract the original (uncentered) item mean location from the person locations:
person.locations_JMLE$theta_adjusted <- person.locations_JMLE$theta - mean(uncentered.item.locations_JMLE)

# Summary of person location estimates:
summary(person.locations_JMLE)
```

If analysts do not prefer to use the zero-centered item location estimates, they can use the original person parameters without subtracting the mean item location.

***Person Fit***

Next, we will examine numeric person fit indices using the `tam.fit()` function from *TAM.* We will save the results in an object called `person.fit_JMLE` and then view summary statistics for the fit statistics.

```{r}
JMLE_fit <- tam.fit(PCM.science_JMLE)

person.fit_JMLE <- JMLE_fit$fit.person

summary(person.fit_JMLE)
```

The `tam.fit()` function provides mean square error ($MSE$) and standardized ($t$) versions of the outfit and infit statistics for each person location estimate.

***Wright Map***

Finally, we will create a Wright Map from our model results. This display will provide us with an overview of the distribution of person, item, and threshold parameters. We will create the plot by applying the *WrightMap* package function `IRT.WrightMap()` to the model object (`PCM.science_JMLE`). 

For ease in interpretation, we will use the centered item and person locations that we calculated in this analysis. We need to specify these modified parameter estimates in the `WrightMap()` function. The following code prepares the parameter estimates and plots the Wright Map using them.

```{r}
# Combine centered item estimates with thresholds:
n.items <- ncol(science.responses)

thresholds_JMLE <- matrix(data = NA, nrow = n.items, ncol = n.thresholds)

for(item.number in 1:n.items){
  for(tau in 1:n.thresholds){
  thresholds_JMLE[item.number, tau] <- centered.item.locations_JMLE[item.number] +
      tau.matrix_JMLE[item.number, tau]
  } 
}

thetas_JMLE <- person.locations_JMLE$theta_adjusted

# Plot the Wrighy Map
wrightMap(thetas = thetas_JMLE,
        thresholds = thresholds_JMLE,
         main.title = "Liking for Science Partial Credit Model Wright Map (JMLE)",
        show.thr.lab	= TRUE, dim.names = "",
        label.items.rows= 2)
```

## Example results section

```{r}
# Print Table 1:
knitr::kable(
  PCM_Table1, booktabs = TRUE,
  caption = 'Model Summary Table'
)
```

Table 1 presents a summary of the results from the analysis of the Liking for Science data [@RS_analysis] using the Partial Credit Model (PCM) [@PCM]. One child was excluded from the analysis (Child #2), who gave extreme responses to all of the items ($x = 2$).

Specifically, Table 1 summarizes the calibration of the children with non-extreme responses ($N = 74$) and the items ($N = 25$) using average logit-scale calibrations, standard errors, and model-data fit statistics. Examination of the results indicates that, on average, the children were located higher on the logit scale ($M = 0.96$,$SD = 1.19$), compared to items, whose locations were centered at zero logits ($M = 0.00$, $SD = 1.20$). This finding suggests that the items were relatively easy to endorse among the sample of children who participated in this study. The average values of the Standard Error ($SE$) were comparable for children ($M = 0.38$) and items ($M = 0.40$). Average values of model-data fit statistics indicate overall adequate fit to the model, with average outfit and infit mean square error ($MSE$) statistics around 1.00, and average standardized outfit and infit statistics near the expected value of 0.00 when data fit the model. However, there was some variability in item fit and person fit, as indicated by relatively large standard deviations for the fit statistics. Additional investigation into item fit and person fit is warranted.

```{r}
# Print Table 2:
knitr::kable(
  PCM_Table2, booktabs = TRUE,
  caption = 'Item Calibrations'
)
```

Table 2 includes detailed results for the 25 Liking for Science items, where items are ordered by their overall logit-scale location (i.e., item difficulty) from high (difficult to endorse) to low (easy to endorse). For each item, the average rating is presented, followed by the overall logit-scale location ($\delta$), the location of the item-specific rating scale category thresholds, and item fit statistics. Item 5 was the most difficult to endorse ($Average Rating = 0.49$; $\delta$ = 2.08), followed by Item 23 ($Average Rating = 0.56$; $\delta$ = 1.85). The easiest item to endorse was Item 18 ($Average Rating = 1.93$; $\delta$ = -2.13).

```{r}
# Print Table 3:
knitr::kable(
  head(PCM_Table3,10), booktabs = TRUE,
  caption = 'Person Calibration'
)
```

Table 3 includes detailed results for 10 children who participated in the Liking for Science survey. For each child, the average rating is presented, followed by their logit-scale location estimate ($\theta$), $SE$, and model-data fit statistics. 

Figure 1
```{r}
graphics.off()
plotPImap(PCM.science, main = "Liking for Science Partial Credit Model Wright Map", sorted = TRUE, irug = FALSE)
```

Figure 1 illustrates the calibrations of the children and items on the logit scale that represents the latent variable. The calibrations shown in this figure correspond to the results presented in Table 2 and Table 3 for items and children, respectively. Starting at the bottom of the figure, the horizontal axis (labeled *Latent Dimension*) is the logit scale that represents the latent variable. Lower numbers on this scale indicate less-favorable attitudes toward science activities, and higher numbers indicate more-favorable attitudes toward science activities. The central panel of the figure shows item difficulty locations on the logit scale for the 25 Liking for Science items included in the analysis; the y-axis for this panel shows the item labels. The items are ordered according to their difficulty order, as estimated with the PCM. Easier-to-endorse items appear at the top of the figure, and harder-to-endorse items appear at the bottom of the figure; item labels are shown on the y-axis. For each item, a solid circle plotting symbol shows the overall location estimate. This solid circle symbol is connected to two open-circle symbols that show the locations of the rating scale category thresholds. Each threshold is labeled with a $1$ to indicate the threshold between rating scale category $x = 0$ and $x = 1$, or a $2$ to indicate the threshold between rating scale category $x* = 1$ and $x = 2$. The threshold locations were ordered as expected for most items, with $\tau_1$ located lower on the logit scale ($\tau$) compared to the location estimate for $\tau_2$. However, the thresholds were disordered for Item 13 and Item 18. In addition, there were some differences in the relative difficulty to endorse the rating scale categories across items. Finally, the upper panel of the figure shows a histogram of person (in this case, children) location estimates on the logit scale.


## Exercise

Please use the *eRm* or *TAM* package to estimate item, threshold, and person locations with the PCM for the Exercise 5 data. The Exercise 5 data include responses from 500 participants to an attitude survey that includes 20 items with a 5-category rating scale. This dataset is the same as the Exercise 4 data. After completing the analysis, try writing a results section similar to the example in this chapter based on your findings.

